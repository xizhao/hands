---
title: Build a Data Pipeline
description: "Automate data processing with scheduled jobs"
---

# Build a Data Pipeline

In this guide, you'll create an automated data pipeline that:
- Syncs data from an external API
- Transforms and aggregates data
- Generates daily reports

## What You'll Build

- A polling integration to fetch external data
- Transform jobs to process raw data
- Aggregation jobs for analytics
- A report generation job

## Step 1: Set Up the Data Source

First, create tables for raw and processed data:

```
Create tables for a data pipeline:
- raw_events: stores raw API data with timestamp
- processed_events: cleaned and enriched data
- daily_stats: aggregated metrics by day
```

The AI creates:

```sql
CREATE TABLE raw_events (
  id SERIAL PRIMARY KEY,
  external_id VARCHAR(255) UNIQUE,
  payload JSONB NOT NULL,
  fetched_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE processed_events (
  id SERIAL PRIMARY KEY,
  raw_id INTEGER REFERENCES raw_events(id),
  event_type VARCHAR(100),
  user_id VARCHAR(255),
  value NUMERIC,
  occurred_at TIMESTAMP,
  processed_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE daily_stats (
  date DATE PRIMARY KEY,
  event_count INTEGER,
  unique_users INTEGER,
  total_value NUMERIC,
  computed_at TIMESTAMP DEFAULT NOW()
);
```

## Step 2: Create the Sync Integration

```
Create an integration that fetches events from an API every 30 minutes
```

```typescript
// integrations/sync-events.ts
import { integration } from "@hands/stdlib";

export const handler = integration(async (ctx) => {
  // Get last sync timestamp
  const [{ max_id }] = await ctx.sql`
    SELECT COALESCE(MAX(external_id), '0') as max_id FROM raw_events
  `;

  // Fetch new events from API
  const response = await fetch(
    `${ctx.env.API_URL}/events?after=${max_id}`,
    {
      headers: { "Authorization": `Bearer ${ctx.env.API_KEY}` }
    }
  );

  if (!response.ok) {
    return { status: 500, body: { error: "API request failed" } };
  }

  const events = await response.json();

  // Insert into raw_events
  for (const event of events) {
    await ctx.sql`
      INSERT INTO raw_events (external_id, payload)
      VALUES (${event.id}, ${JSON.stringify(event)})
      ON CONFLICT (external_id) DO NOTHING
    `;
  }

  ctx.log(`Synced ${events.length} events`);

  return { status: 200, body: { synced: events.length } };
});
```

## Step 3: Create the Transform Job

```
Create a monitor that processes raw events every 15 minutes
```

```typescript
// monitors/process-events.ts
import { monitor } from "@hands/stdlib";

export const handler = monitor(async (ctx) => {
  // Find unprocessed events
  const unprocessed = await ctx.sql`
    SELECT * FROM raw_events r
    WHERE NOT EXISTS (
      SELECT 1 FROM processed_events p WHERE p.raw_id = r.id
    )
    LIMIT 1000
  `;

  let processed = 0;
  let errors = 0;

  for (const raw of unprocessed) {
    try {
      const payload = raw.payload;

      // Transform the data
      await ctx.sql`
        INSERT INTO processed_events (raw_id, event_type, user_id, value, occurred_at)
        VALUES (
          ${raw.id},
          ${payload.type},
          ${payload.user?.id},
          ${parseFloat(payload.value) || 0},
          ${new Date(payload.timestamp)}
        )
      `;

      processed++;
    } catch (error) {
      ctx.log(`Error processing ${raw.id}: ${error.message}`);
      errors++;
    }
  }

  ctx.log(`Processed ${processed} events, ${errors} errors`);

  return {
    status: errors > 0 ? "warning" : "ok",
    processed,
    errors
  };
});
```

## Step 4: Create the Aggregation Job

```
Create a daily aggregation job that runs at 2 AM
```

```typescript
// monitors/daily-aggregation.ts
import { monitor } from "@hands/stdlib";

export const handler = monitor(async (ctx) => {
  // Calculate stats for yesterday
  const yesterday = new Date();
  yesterday.setDate(yesterday.getDate() - 1);
  const dateStr = yesterday.toISOString().split('T')[0];

  const [stats] = await ctx.sql`
    SELECT
      COUNT(*) as event_count,
      COUNT(DISTINCT user_id) as unique_users,
      SUM(value) as total_value
    FROM processed_events
    WHERE DATE(occurred_at) = ${dateStr}
  `;

  // Upsert into daily_stats
  await ctx.sql`
    INSERT INTO daily_stats (date, event_count, unique_users, total_value)
    VALUES (${dateStr}, ${stats.event_count}, ${stats.unique_users}, ${stats.total_value})
    ON CONFLICT (date) DO UPDATE SET
      event_count = EXCLUDED.event_count,
      unique_users = EXCLUDED.unique_users,
      total_value = EXCLUDED.total_value,
      computed_at = NOW()
  `;

  ctx.log(`Aggregated stats for ${dateStr}: ${stats.event_count} events`);

  return { status: "ok", date: dateStr, stats };
});
```

## Step 5: Configure the Pipeline

```typescript
// hands.config.ts
import { defineConfig } from "@hands/stdlib";

export default defineConfig({
  name: "data-pipeline",

  integrations: {
    "sync-events": {
      type: "polling",
      schedule: "rate(30 minutes)",
      handler: "integrations/sync-events.handler"
    }
  },

  monitors: {
    "process-events": {
      schedule: "rate(15 minutes)",
      handler: "monitors/process-events.handler",
      timeout: 120  // 2 minutes for batch processing
    },

    "daily-aggregation": {
      schedule: "cron(0 2 * * *)",  // 2 AM daily
      handler: "monitors/daily-aggregation.handler"
    }
  }
});
```

## Step 6: Add Monitoring

```
Create a dashboard showing:
- Sync status (last sync time, events pending)
- Processing backlog
- Daily stats trend
- Error count
```

## Step 7: Add Error Handling

```
Add a dead letter queue for failed events
```

```typescript
// Create table
await ctx.sql`
  CREATE TABLE IF NOT EXISTS failed_events (
    id SERIAL PRIMARY KEY,
    raw_id INTEGER REFERENCES raw_events(id),
    error TEXT,
    failed_at TIMESTAMP DEFAULT NOW(),
    retries INTEGER DEFAULT 0
  )
`;

// In the process job, on error:
await ctx.sql`
  INSERT INTO failed_events (raw_id, error)
  VALUES (${raw.id}, ${error.message})
`;
```

## Step 8: Deploy

```
Deploy the pipeline to Cloudflare
```

Don't forget to set secrets:

```bash
wrangler secret put API_KEY
wrangler secret put API_URL
```

## Pipeline Visualization

```
┌─────────────┐    ┌──────────────┐    ┌───────────────┐    ┌────────────┐
│ External    │───▶│ raw_events   │───▶│ process-events│───▶│ processed  │
│ API         │    │ (sync every  │    │ (every 15min) │    │ _events    │
└─────────────┘    │  30min)      │    └───────────────┘    └────────────┘
                   └──────────────┘                                │
                                                                   ▼
                                                          ┌───────────────┐
                                                          │ daily-agg     │
                                                          │ (2 AM daily)  │
                                                          └───────────────┘
                                                                   │
                                                                   ▼
                                                          ┌────────────┐
                                                          │ daily_stats│
                                                          └────────────┘
```

## What's Next?

<CardGroup cols={2}>
  <Card title="API Backend" icon="code" href="/guides/api-backend">
    Expose your processed data via API
  </Card>
  <Card title="Scheduled Jobs" icon="clock" href="/apps/scheduled-jobs">
    Learn more about monitors
  </Card>
</CardGroup>
