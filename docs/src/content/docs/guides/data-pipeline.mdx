---
title: "Guide: Building a Data Pipeline"
description: Create automated data processing with scheduled jobs
---

import { Steps } from '@astrojs/starlight/components';

This guide shows you how to build a data pipeline that syncs data from external sources, transforms it, and generates reports.

## What we'll build

A pipeline that:
1. Syncs customer data from an external API
2. Enriches it with calculated fields
3. Generates daily summary statistics
4. Sends alerts for anomalies

## Step 1: Set up the database schema

Create tables for our pipeline:

```sql
-- Raw customer data from external source
CREATE TABLE customers_raw (
  id INTEGER PRIMARY KEY,
  name TEXT,
  email TEXT,
  company TEXT,
  created_at TIMESTAMP,
  metadata JSONB,
  synced_at TIMESTAMP DEFAULT NOW()
);

-- Enriched customer data
CREATE TABLE customers (
  id INTEGER PRIMARY KEY,
  name TEXT,
  email TEXT,
  company TEXT,
  domain TEXT,            -- Extracted from email
  company_size TEXT,      -- Derived from metadata
  customer_segment TEXT,  -- Calculated
  lifetime_value NUMERIC,
  created_at TIMESTAMP,
  updated_at TIMESTAMP DEFAULT NOW()
);

-- Daily summary statistics
CREATE TABLE daily_stats (
  date DATE PRIMARY KEY,
  new_customers INTEGER,
  total_customers INTEGER,
  avg_lifetime_value NUMERIC,
  calculated_at TIMESTAMP DEFAULT NOW()
);

-- Sync log for tracking
CREATE TABLE sync_log (
  id SERIAL PRIMARY KEY,
  job_name TEXT,
  status TEXT,
  records_processed INTEGER,
  error_message TEXT,
  started_at TIMESTAMP,
  completed_at TIMESTAMP
);
```

## Step 2: Create the sync job

Create a job to fetch data from an external API:

```typescript
// src/jobs/sync-customers.ts
import { sql, integration } from '@hands/stdlib';

const API_URL = process.env.CUSTOMER_API_URL;
const API_KEY = process.env.CUSTOMER_API_KEY;

export default async function syncCustomers() {
  const startTime = new Date();
  let recordsProcessed = 0;

  try {
    // Get last sync time
    const lastSync = await sql`
      SELECT MAX(synced_at) as last FROM customers_raw
    `;
    const since = lastSync[0]?.last || '1970-01-01';

    // Fetch new/updated records from API
    const response = await fetch(
      `${API_URL}/customers?updated_since=${since}`,
      { headers: { 'Authorization': `Bearer ${API_KEY}` } }
    );
    const customers = await response.json();

    // Upsert into raw table
    for (const customer of customers) {
      await sql`
        INSERT INTO customers_raw (id, name, email, company, created_at, metadata)
        VALUES (
          ${customer.id},
          ${customer.name},
          ${customer.email},
          ${customer.company},
          ${customer.created_at},
          ${JSON.stringify(customer.metadata)}
        )
        ON CONFLICT (id) DO UPDATE SET
          name = EXCLUDED.name,
          email = EXCLUDED.email,
          company = EXCLUDED.company,
          metadata = EXCLUDED.metadata,
          synced_at = NOW()
      `;
      recordsProcessed++;
    }

    // Log success
    await sql`
      INSERT INTO sync_log (job_name, status, records_processed, started_at, completed_at)
      VALUES ('sync-customers', 'success', ${recordsProcessed}, ${startTime}, NOW())
    `;

    console.log(`Synced ${recordsProcessed} customers`);

  } catch (error) {
    // Log failure
    await sql`
      INSERT INTO sync_log (job_name, status, error_message, started_at, completed_at)
      VALUES ('sync-customers', 'failed', ${error.message}, ${startTime}, NOW())
    `;
    throw error;
  }
}
```

## Step 3: Create the transformation job

Transform raw data into enriched format:

```typescript
// src/jobs/transform-customers.ts
import { sql } from '@hands/stdlib';

export default async function transformCustomers() {
  // Process new/updated raw records
  const rawCustomers = await sql`
    SELECT * FROM customers_raw
    WHERE synced_at > (
      SELECT COALESCE(MAX(updated_at), '1970-01-01')
      FROM customers
    )
  `;

  for (const raw of rawCustomers) {
    // Extract domain from email
    const domain = raw.email?.split('@')[1] || null;

    // Determine company size from metadata
    const employeeCount = raw.metadata?.employee_count || 0;
    const companySize =
      employeeCount > 1000 ? 'enterprise' :
      employeeCount > 100 ? 'mid-market' :
      employeeCount > 10 ? 'small-business' : 'startup';

    // Calculate customer segment
    const segment = await calculateSegment(raw.id);

    // Calculate lifetime value
    const ltv = await sql`
      SELECT COALESCE(SUM(amount), 0) as total
      FROM orders WHERE customer_id = ${raw.id}
    `;

    // Upsert enriched record
    await sql`
      INSERT INTO customers (
        id, name, email, company, domain,
        company_size, customer_segment, lifetime_value,
        created_at, updated_at
      )
      VALUES (
        ${raw.id}, ${raw.name}, ${raw.email}, ${raw.company}, ${domain},
        ${companySize}, ${segment}, ${ltv[0].total},
        ${raw.created_at}, NOW()
      )
      ON CONFLICT (id) DO UPDATE SET
        name = EXCLUDED.name,
        email = EXCLUDED.email,
        company = EXCLUDED.company,
        domain = EXCLUDED.domain,
        company_size = EXCLUDED.company_size,
        customer_segment = EXCLUDED.customer_segment,
        lifetime_value = EXCLUDED.lifetime_value,
        updated_at = NOW()
    `;
  }

  console.log(`Transformed ${rawCustomers.length} customers`);
}

async function calculateSegment(customerId: number): Promise<string> {
  const metrics = await sql`
    SELECT
      COUNT(*) as order_count,
      SUM(amount) as total_spent,
      MAX(created_at) as last_order
    FROM orders
    WHERE customer_id = ${customerId}
  `;

  const m = metrics[0];
  if (m.total_spent > 10000) return 'vip';
  if (m.order_count > 10) return 'loyal';
  if (m.last_order > new Date(Date.now() - 30 * 24 * 60 * 60 * 1000)) return 'active';
  return 'standard';
}
```

## Step 4: Create the aggregation job

Generate daily statistics:

```typescript
// src/jobs/aggregate-stats.ts
import { sql } from '@hands/stdlib';

export default async function aggregateStats() {
  // Calculate yesterday's stats
  const yesterday = await sql`
    SELECT CURRENT_DATE - 1 as date
  `;
  const date = yesterday[0].date;

  // Check if already calculated
  const existing = await sql`
    SELECT 1 FROM daily_stats WHERE date = ${date}
  `;
  if (existing.length > 0) {
    console.log(`Stats for ${date} already exist`);
    return;
  }

  // Calculate metrics
  const stats = await sql`
    SELECT
      COUNT(*) FILTER (WHERE DATE(created_at) = ${date}) as new_customers,
      COUNT(*) as total_customers,
      AVG(lifetime_value) as avg_lifetime_value
    FROM customers
    WHERE created_at <= ${date} + INTERVAL '1 day'
  `;

  // Insert stats
  await sql`
    INSERT INTO daily_stats (date, new_customers, total_customers, avg_lifetime_value)
    VALUES (
      ${date},
      ${stats[0].new_customers},
      ${stats[0].total_customers},
      ${stats[0].avg_lifetime_value}
    )
  `;

  console.log(`Generated stats for ${date}`);
}
```

## Step 5: Create the alerting job

Send alerts for anomalies:

```typescript
// src/jobs/check-alerts.ts
import { sql } from '@hands/stdlib';

const SLACK_WEBHOOK = process.env.SLACK_WEBHOOK_URL;

export default async function checkAlerts() {
  // Compare today vs 7-day average
  const comparison = await sql`
    WITH recent AS (
      SELECT AVG(new_customers) as avg_new
      FROM daily_stats
      WHERE date > CURRENT_DATE - 8
        AND date < CURRENT_DATE
    ),
    today AS (
      SELECT new_customers
      FROM daily_stats
      WHERE date = CURRENT_DATE - 1
    )
    SELECT
      today.new_customers,
      recent.avg_new,
      today.new_customers - recent.avg_new as diff,
      (today.new_customers - recent.avg_new) / NULLIF(recent.avg_new, 0) * 100 as pct_change
    FROM today, recent
  `;

  const data = comparison[0];
  if (!data) return;

  // Alert if significant change
  if (Math.abs(data.pct_change) > 30) {
    const direction = data.pct_change > 0 ? 'increased' : 'decreased';
    const message = `New customers ${direction} by ${Math.abs(data.pct_change).toFixed(1)}% ` +
                   `(${data.new_customers} vs ${data.avg_new.toFixed(0)} avg)`;

    await fetch(SLACK_WEBHOOK, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text: message })
    });

    console.log(`Alert sent: ${message}`);
  }
}
```

## Step 6: Configure the schedule

Set up the pipeline schedule in `hands.config.ts`:

```typescript
import { defineConfig } from '@hands/stdlib';

export default defineConfig({
  name: 'customer-pipeline',

  triggers: {
    scheduled: [
      // Sync from external API every 15 minutes
      {
        cron: '*/15 * * * *',
        handler: './src/jobs/sync-customers.ts'
      },
      // Transform data every hour
      {
        cron: '5 * * * *',
        handler: './src/jobs/transform-customers.ts'
      },
      // Aggregate stats daily at 1 AM
      {
        cron: '0 1 * * *',
        handler: './src/jobs/aggregate-stats.ts'
      },
      // Check for alerts at 9 AM
      {
        cron: '0 9 * * *',
        handler: './src/jobs/check-alerts.ts'
      }
    ]
  }
});
```

## Step 7: Add monitoring dashboard

Create a dashboard to monitor the pipeline:

```typescript
// src/dashboards/pipeline.ts
import { dashboard } from '@hands/stdlib';

export default dashboard({
  title: 'Pipeline Monitor',
  refreshInterval: 60,

  charts: [
    {
      title: 'Sync Status',
      type: 'table',
      query: `
        SELECT job_name, status, records_processed, completed_at
        FROM sync_log
        ORDER BY completed_at DESC
        LIMIT 20
      `
    },
    {
      title: 'Records Synced (24h)',
      type: 'line',
      query: `
        SELECT
          DATE_TRUNC('hour', completed_at) as date,
          SUM(records_processed) as value
        FROM sync_log
        WHERE completed_at > NOW() - INTERVAL '24 hours'
          AND status = 'success'
        GROUP BY 1 ORDER BY 1
      `
    },
    {
      title: 'Daily New Customers',
      type: 'bar',
      query: `
        SELECT date::text as name, new_customers as value
        FROM daily_stats
        ORDER BY date DESC
        LIMIT 14
      `
    }
  ]
});
```

## Step 8: Deploy

<Steps>
1. Click **Deploy** to push to Cloudflare
2. Set secrets: `wrangler secret put CUSTOMER_API_KEY`
3. Verify cron triggers are active in Cloudflare dashboard
</Steps>

## Next steps

- Add more [Scheduled jobs](/apps/scheduled-jobs/)
- Build [API routes](/apps/api-routes/) for the processed data
- Create [Dashboards](/apps/dashboards/) for insights
